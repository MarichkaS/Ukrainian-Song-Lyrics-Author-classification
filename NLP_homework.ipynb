{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from stop_words import safe_get_stop_words\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_PATH_LBL = './train/labels.json'\n",
    "TRAIN_PATH_TXT = './train/texts/'\n",
    "TEST_PATH_LBL = './test/labels.json'\n",
    "TEST_PATH_TXT = './test/texts/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0 - Tartak, 1 - Elza's Ocean\n",
    "df_names = ['dict']\n",
    "train = pd.DataFrame(pd.read_json(TRAIN_PATH_LBL, typ='series'), columns=df_names).reset_index()\n",
    "train.columns = ['song', 'dict'] \n",
    "test = pd.DataFrame(pd.read_json(TEST_PATH_LBL, typ='series'), columns=df_names).reset_index()\n",
    "test.columns = ['song', 'dict']\n",
    "\n",
    "y_true = [0 if 'Тартак' in band.values() else 1 for band in train['dict']]\n",
    "train['band'] = pd.Series(y_true)\n",
    "\n",
    "y_test = [0 if 'Тартак' in band.values() else 1 for band in test['dict']]\n",
    "test['band'] = pd.Series(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>dict</th>\n",
       "      <th>band</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001.txt</td>\n",
       "      <td>{'artist': 'Тартак', 'title': '100-ий плагіат'}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002.txt</td>\n",
       "      <td>{'artist': 'Тартак', 'title': '3Dенс'}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003.txt</td>\n",
       "      <td>{'artist': 'Тартак', 'title': 'DJ Вовочка'}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004.txt</td>\n",
       "      <td>{'artist': 'Тартак', 'title': 'Mikpoffonha пер...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005.txt</td>\n",
       "      <td>{'artist': 'Тартак', 'title': 'Qарпа-ратів'}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      song                                               dict  band\n",
       "0  001.txt    {'artist': 'Тартак', 'title': '100-ий плагіат'}     0\n",
       "1  002.txt             {'artist': 'Тартак', 'title': '3Dенс'}     0\n",
       "2  003.txt        {'artist': 'Тартак', 'title': 'DJ Вовочка'}     0\n",
       "3  004.txt  {'artist': 'Тартак', 'title': 'Mikpoffonha пер...     0\n",
       "4  005.txt       {'artist': 'Тартак', 'title': 'Qарпа-ратів'}     0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_series = pd.DataFrame()\n",
    "text_test_series = pd.DataFrame()\n",
    "\n",
    "for song in train['song']:\n",
    "    content = ''\n",
    "    with open(TRAIN_PATH_TXT + song, encoding='utf-8') as f:\n",
    "        line = f.read().replace('\\n', ' ')\n",
    "    text_series = text_series.append(pd.Series(line), ignore_index=True)\n",
    "    \n",
    "for s in test['song']:\n",
    "    content = ''\n",
    "    with open(TEST_PATH_TXT + s, encoding='utf-8') as f:\n",
    "        line = f.read().replace('\\n', ' ')\n",
    "    text_test_series = text_test_series.append(pd.Series(line), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Дивлюсь на себе в дзеркало й не можу зрозуміти...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Я танцюю І в цьому танці я існую! Мої вуха Пер...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Зранку, лиш розкрию очі, Через жінку перескочу...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Увага! Увага! Небезпечна лаба! Що за?! Що за?!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Від самого ранку весь офіс гудів – Фірма влашт...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  Дивлюсь на себе в дзеркало й не можу зрозуміти...\n",
       "1  Я танцюю І в цьому танці я існую! Мої вуха Пер...\n",
       "2  Зранку, лиш розкрию очі, Через жінку перескочу...\n",
       "3  Увага! Увага! Небезпечна лаба! Що за?! Що за?!...\n",
       "4  Від самого ранку весь офіс гудів – Фірма влашт..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['Text'] = text_series[0]\n",
    "test['Text'] = text_test_series[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>dict</th>\n",
       "      <th>band</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001.txt</td>\n",
       "      <td>{'artist': 'Тартак', 'title': 'Зима хвора'}</td>\n",
       "      <td>0</td>\n",
       "      <td>Крижинки, як хмари – бачиш їх – з неба падають...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002.txt</td>\n",
       "      <td>{'artist': 'Тартак', 'title': 'Ілюзія'}</td>\n",
       "      <td>0</td>\n",
       "      <td>У місті жорстокому, у місті розлюченому Я зажи...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003.txt</td>\n",
       "      <td>{'artist': 'Тартак', 'title': 'Я не знаю'}</td>\n",
       "      <td>0</td>\n",
       "      <td>Я не знаю, що робити... Я не знаю, що казати.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004.txt</td>\n",
       "      <td>{'artist': 'Тартак', 'title': 'Майже на'}</td>\n",
       "      <td>0</td>\n",
       "      <td>Ох, це вже нездолиме прагнення Нового здобуття...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005.txt</td>\n",
       "      <td>{'artist': 'Тартак', 'title': 'Нікому то не тр...</td>\n",
       "      <td>0</td>\n",
       "      <td>Попадали на землю Всі ті, шо я придумав, слова...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      song                                               dict  band  \\\n",
       "0  001.txt        {'artist': 'Тартак', 'title': 'Зима хвора'}     0   \n",
       "1  002.txt            {'artist': 'Тартак', 'title': 'Ілюзія'}     0   \n",
       "2  003.txt         {'artist': 'Тартак', 'title': 'Я не знаю'}     0   \n",
       "3  004.txt          {'artist': 'Тартак', 'title': 'Майже на'}     0   \n",
       "4  005.txt  {'artist': 'Тартак', 'title': 'Нікому то не тр...     0   \n",
       "\n",
       "                                                Text  \n",
       "0  Крижинки, як хмари – бачиш їх – з неба падають...  \n",
       "1  У місті жорстокому, у місті розлюченому Я зажи...  \n",
       "2  Я не знаю, що робити... Я не знаю, що казати.....  \n",
       "3  Ох, це вже нездолиме прагнення Нового здобуття...  \n",
       "4  Попадали на землю Всі ті, шо я придумав, слова...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>dict</th>\n",
       "      <th>band</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001.txt</td>\n",
       "      <td>{'artist': 'Тартак', 'title': '100-ий плагіат'}</td>\n",
       "      <td>0</td>\n",
       "      <td>Дивлюсь на себе в дзеркало й не можу зрозуміти...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002.txt</td>\n",
       "      <td>{'artist': 'Тартак', 'title': '3Dенс'}</td>\n",
       "      <td>0</td>\n",
       "      <td>Я танцюю І в цьому танці я існую! Мої вуха Пер...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003.txt</td>\n",
       "      <td>{'artist': 'Тартак', 'title': 'DJ Вовочка'}</td>\n",
       "      <td>0</td>\n",
       "      <td>Зранку, лиш розкрию очі, Через жінку перескочу...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004.txt</td>\n",
       "      <td>{'artist': 'Тартак', 'title': 'Mikpoffonha пер...</td>\n",
       "      <td>0</td>\n",
       "      <td>Увага! Увага! Небезпечна лаба! Що за?! Що за?!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005.txt</td>\n",
       "      <td>{'artist': 'Тартак', 'title': 'Qарпа-ратів'}</td>\n",
       "      <td>0</td>\n",
       "      <td>Від самого ранку весь офіс гудів – Фірма влашт...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      song                                               dict  band  \\\n",
       "0  001.txt    {'artist': 'Тартак', 'title': '100-ий плагіат'}     0   \n",
       "1  002.txt             {'artist': 'Тартак', 'title': '3Dенс'}     0   \n",
       "2  003.txt        {'artist': 'Тартак', 'title': 'DJ Вовочка'}     0   \n",
       "3  004.txt  {'artist': 'Тартак', 'title': 'Mikpoffonha пер...     0   \n",
       "4  005.txt       {'artist': 'Тартак', 'title': 'Qарпа-ратів'}     0   \n",
       "\n",
       "                                                Text  \n",
       "0  Дивлюсь на себе в дзеркало й не можу зрозуміти...  \n",
       "1  Я танцюю І в цьому танці я існую! Мої вуха Пер...  \n",
       "2  Зранку, лиш розкрию очі, Через жінку перескочу...  \n",
       "3  Увага! Увага! Небезпечна лаба! Що за?! Що за?!...  \n",
       "4  Від самого ранку весь офіс гудів – Фірма влашт...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = text_series.iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uk'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#detecting language of the given sentence\n",
    "text = TextBlob(t)\n",
    "text.detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Дивлюсь', 'на', 'себе', 'в', 'дзеркало', 'й', 'не', 'можу', 'зрозуміти', 'Чому', 'я', 'такий', 'гарний', 'і', 'чому', 'такий', 'талановитий', 'Вигадую', 'мелодії', 'гармонії', 'чарівної', 'І', 'філософські', 'тексти', 'глибини', 'неймовірної', 'Я', 'хочу', 'щоб', 'мій', 'голос', 'усі', 'пізнавали', 'Мої', 'пісні', 'співали', 'усі', 'слова', 'напам', '’', 'ять', 'знали', 'Я', 'хочу', 'захлинутися', 'славою', 'негіркою', 'Я', 'хочу', 'стати', 'справжньою', 'супер-пупер-зіркою', 'Але', 'щоб', 'усе', 'це', 'мати', 'Треба', 'зовсім', 'небагато', 'Прокрутіть', 'цю', 'пісню', 'по', 'радіо', 'Прокрутіть', 'цю', 'пісню', 'по', 'радіо', 'О-о-о', 'Прокрутіть', 'цю', 'пісню', 'по', 'радіо', 'Прокрутіть', 'цю', 'пісню', 'по', 'радіо', 'О-о-о', 'Я', 'хочу', 'заробляти', 'великі-превеликі', 'гроші', 'Бо', 'хоч', 'вони', 'погано', 'пахнуть', 'але', 'такі', 'хороші', 'Кататися', 'по', 'місту', 'в', 'блискучому', 'довгому', 'лімузині', 'Й', 'купувати', 'речі', 'в', 'дорогому', 'магазині', 'Я', 'хочу', 'щоб', 'мене', 'любили', 'високі', 'фотомоделі', 'Щоб', 'з', 'ними', 'розважатися', 'в', 'п', '’', 'ятизірковому', 'готелі', 'Повісити', 'на', 'шию', 'три', 'кілограми', 'золота', 'І', 'щоб', 'в', 'житті', 'не', 'брати', 'в', 'руки', 'ні', 'серпа', 'ні', 'молота', 'Але', 'щоб', 'усе', 'це', 'мати', 'Треба', 'зовсім', 'небагато', 'Прокрутіть', 'цю', 'пісню', 'по', 'радіо', 'Прокрутіть', 'цю', 'пісню', 'по', 'радіо', 'О-о-о', 'Прокрутіть', 'цю', 'пісню', 'по', 'радіо', 'Прокрутіть', 'цю', 'пісню', 'по', 'радіо', 'О-о-о', 'Не', 'все', 'в', 'житті', 'так', 'просто', 'не', 'все', 'в', 'житті', 'так', 'ясно', '–', 'Приніс', 'пісні', 'на', 'радіо', 'та', 'як', 'завжди', 'приніс', 'невчасно', 'Негідного', 'формату', 'компромату', 'небагато', 'І', 'жодного', 'нема', 'фірмового', 'гітарного', 'квадрату', 'Ну', 'як', 'без', 'плагіату', 'Ну', 'хіба', 'ж', 'так', 'можна', 'грати', 'Нам', 'потрібно', 'регулярно', 'стрімко', 'рейтинг', 'підіймати', 'А', 'ви', 'не', 'відрізняєтесь', 'приємними', 'манерами', 'До', 'того', 'ж', 'ваша', 'творчість', 'не', 'підкріплена', 'паперами', 'Ну', 'прокрутіть', 'цю', 'пісню', 'по', 'радіо', 'О-о', 'О-о', 'Прокрутіть', 'цю', 'пісню', 'по', 'радіо', 'О-о-о', 'Прокрутіть', 'цю', 'пісню', 'по', 'радіо', 'О-о', 'О-о', 'Прокрутіть', 'цю', 'пісню', 'по', 'радіо', 'О-о-о'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's define tokens for the given sentence\n",
    "tokens_t = TextBlob(t)\n",
    "tokens_t.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"Дивлюсь на себе в дзеркало й не можу зрозуміти, Чому я такий гарний і чому такий талановитий?\"),\n",
       " Sentence(\"Вигадую мелодії гармонії чарівної І філософські тексти глибини неймовірної.\"),\n",
       " Sentence(\"Я хочу, щоб мій голос усі пізнавали, Мої пісні співали, усі слова напам’ять знали.\"),\n",
       " Sentence(\"Я хочу захлинутися славою негіркою, Я хочу стати справжньою супер-пупер-зіркою!\"),\n",
       " Sentence(\"Але щоб усе це мати, Треба зовсім небагато!\"),\n",
       " Sentence(\"Прокрутіть цю пісню по радіо!\"),\n",
       " Sentence(\"Прокрутіть цю пісню по радіо!\"),\n",
       " Sentence(\"О-о-о!\"),\n",
       " Sentence(\"Прокрутіть цю пісню по радіо!\"),\n",
       " Sentence(\"Прокрутіть цю пісню по радіо!\"),\n",
       " Sentence(\"О-о-о!\"),\n",
       " Sentence(\"Я хочу заробляти великі-превеликі гроші, Бо хоч вони погано пахнуть, але такі хороші!\"),\n",
       " Sentence(\"Кататися по місту в блискучому довгому лімузині, Й купувати речі в дорогому магазині.\"),\n",
       " Sentence(\"Я хочу, щоб мене любили високі фотомоделі, Щоб з ними розважатися в п’ятизірковому готелі.\"),\n",
       " Sentence(\"Повісити на шию три кілограми золота І щоб в житті не брати в руки ні серпа, ні молота!\"),\n",
       " Sentence(\"Але щоб усе це мати, Треба зовсім небагато!\"),\n",
       " Sentence(\"Прокрутіть цю пісню по радіо!\"),\n",
       " Sentence(\"Прокрутіть цю пісню по радіо!\"),\n",
       " Sentence(\"О-о-о!\"),\n",
       " Sentence(\"Прокрутіть цю пісню по радіо!\"),\n",
       " Sentence(\"Прокрутіть цю пісню по радіо!\"),\n",
       " Sentence(\"О-о-о!\"),\n",
       " Sentence(\"Не все в житті так просто, не все в житті так ясно – Приніс пісні на радіо, та, як завжди, приніс невчасно!\"),\n",
       " Sentence(\"Негідного формату, компромату небагато І жодного нема фірмового гітарного квадрату!\"),\n",
       " Sentence(\"Ну, як без плагіату?\"),\n",
       " Sentence(\"Ну, хіба ж так можна грати?\"),\n",
       " Sentence(\"Нам потрібно регулярно стрімко рейтинг підіймати!\"),\n",
       " Sentence(\"А ви не відрізняєтесь приємними манерами, До того ж, ваша творчість не підкріплена паперами!\"),\n",
       " Sentence(\"Ну, прокрутіть цю пісню по радіо!\"),\n",
       " Sentence(\"О-о!\"),\n",
       " Sentence(\"О-о!\"),\n",
       " Sentence(\"Прокрутіть цю пісню по радіо!\"),\n",
       " Sentence(\"О-о-о!\"),\n",
       " Sentence(\"Прокрутіть цю пісню по радіо!\"),\n",
       " Sentence(\"О-о!\"),\n",
       " Sentence(\"О-о!\"),\n",
       " Sentence(\"Прокрутіть цю пісню по радіо!\"),\n",
       " Sentence(\"О-о-о!\")]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split to sentences\n",
    "tokens_t.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = get_stop_words('ukrainian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'б',\n",
       " 'в',\n",
       " 'г',\n",
       " 'е',\n",
       " 'ж',\n",
       " 'з',\n",
       " 'м',\n",
       " 'т',\n",
       " 'у',\n",
       " 'я',\n",
       " 'є',\n",
       " 'і',\n",
       " 'аж',\n",
       " 'ви',\n",
       " 'де',\n",
       " 'до',\n",
       " 'за',\n",
       " 'зі',\n",
       " 'ми',\n",
       " 'на',\n",
       " 'не',\n",
       " 'ну',\n",
       " 'нх',\n",
       " 'ні',\n",
       " 'по',\n",
       " 'та',\n",
       " 'ти',\n",
       " 'то',\n",
       " 'ту',\n",
       " 'ті',\n",
       " 'це',\n",
       " 'цю',\n",
       " 'ця',\n",
       " 'ці',\n",
       " 'чи',\n",
       " 'ще',\n",
       " 'що',\n",
       " 'як',\n",
       " 'їй',\n",
       " 'їм',\n",
       " 'їх',\n",
       " 'її',\n",
       " 'або',\n",
       " 'але',\n",
       " 'ало',\n",
       " 'без',\n",
       " 'був',\n",
       " 'вам',\n",
       " 'вас',\n",
       " 'ваш',\n",
       " 'вже',\n",
       " 'все',\n",
       " 'всю',\n",
       " 'вся',\n",
       " 'від',\n",
       " 'він',\n",
       " 'два',\n",
       " 'дві',\n",
       " 'для',\n",
       " 'ким',\n",
       " 'мож',\n",
       " 'моя',\n",
       " 'моє',\n",
       " 'мої',\n",
       " 'міг',\n",
       " 'між',\n",
       " 'мій',\n",
       " 'над',\n",
       " 'нам',\n",
       " 'нас',\n",
       " 'наш',\n",
       " 'нею',\n",
       " 'неї',\n",
       " 'них',\n",
       " 'ніж',\n",
       " 'ній',\n",
       " 'ось',\n",
       " 'при',\n",
       " 'про',\n",
       " 'під',\n",
       " 'пір',\n",
       " 'раз',\n",
       " 'рік',\n",
       " 'сам',\n",
       " 'сих',\n",
       " 'сім',\n",
       " 'так',\n",
       " 'там',\n",
       " 'теж',\n",
       " 'тим',\n",
       " 'тих',\n",
       " 'той',\n",
       " 'тою',\n",
       " 'три',\n",
       " 'тут',\n",
       " 'хоч',\n",
       " 'хто',\n",
       " 'цей',\n",
       " 'цим',\n",
       " 'цих',\n",
       " 'час',\n",
       " 'щоб',\n",
       " 'яка',\n",
       " 'які',\n",
       " 'адже',\n",
       " 'буде',\n",
       " 'буду',\n",
       " 'будь',\n",
       " 'була',\n",
       " 'були',\n",
       " 'було',\n",
       " 'бути',\n",
       " 'вами',\n",
       " 'ваша',\n",
       " 'ваше',\n",
       " 'ваші',\n",
       " 'весь',\n",
       " 'вниз',\n",
       " 'вона',\n",
       " 'вони',\n",
       " 'воно',\n",
       " 'всею',\n",
       " 'всім',\n",
       " 'всіх',\n",
       " 'втім',\n",
       " 'геть',\n",
       " 'далі',\n",
       " 'двох',\n",
       " 'день',\n",
       " 'дуже',\n",
       " 'зате',\n",
       " 'його',\n",
       " 'йому',\n",
       " 'каже',\n",
       " 'кого',\n",
       " 'коли',\n",
       " 'кому',\n",
       " 'крім',\n",
       " 'куди',\n",
       " 'лише',\n",
       " 'люди',\n",
       " 'мало',\n",
       " 'мати',\n",
       " 'мене',\n",
       " 'мені',\n",
       " 'миру',\n",
       " 'мною',\n",
       " 'може',\n",
       " 'нами',\n",
       " 'наша',\n",
       " 'наше',\n",
       " 'наші',\n",
       " 'ними',\n",
       " 'ніби',\n",
       " 'один',\n",
       " 'поки',\n",
       " 'пора',\n",
       " 'рано',\n",
       " 'року',\n",
       " 'році',\n",
       " 'сама',\n",
       " 'саме',\n",
       " 'саму',\n",
       " 'самі',\n",
       " 'свою',\n",
       " 'своє',\n",
       " 'свої',\n",
       " 'себе',\n",
       " 'собі',\n",
       " 'став',\n",
       " 'суть',\n",
       " 'така',\n",
       " 'таке',\n",
       " 'такі',\n",
       " 'твоя',\n",
       " 'твоє',\n",
       " 'твій',\n",
       " 'тебе',\n",
       " 'тими',\n",
       " 'тобі',\n",
       " 'того',\n",
       " 'тоді',\n",
       " 'тому',\n",
       " 'туди',\n",
       " 'хоча',\n",
       " 'хіба',\n",
       " 'цими',\n",
       " 'цієї',\n",
       " 'часу',\n",
       " 'чого',\n",
       " 'чому',\n",
       " 'який',\n",
       " 'яких',\n",
       " 'якої',\n",
       " 'якщо',\n",
       " \"ім'я\",\n",
       " 'інша',\n",
       " 'інше',\n",
       " 'інші',\n",
       " 'буває',\n",
       " 'будеш',\n",
       " 'більш',\n",
       " 'вгору',\n",
       " 'вміти',\n",
       " 'внизу',\n",
       " 'вісім',\n",
       " 'давно',\n",
       " 'даром',\n",
       " 'добре',\n",
       " 'довго',\n",
       " 'друго',\n",
       " 'дякую',\n",
       " 'життя',\n",
       " 'зараз',\n",
       " 'знову',\n",
       " 'какая',\n",
       " 'кожен',\n",
       " 'кожна',\n",
       " 'кожне',\n",
       " 'кожні',\n",
       " 'краще',\n",
       " 'ледве',\n",
       " 'майже',\n",
       " 'менше',\n",
       " 'могти',\n",
       " 'можна',\n",
       " 'назад',\n",
       " 'немає',\n",
       " 'нижче',\n",
       " 'нього',\n",
       " 'однак',\n",
       " \"п'ять\",\n",
       " 'перед',\n",
       " 'поруч',\n",
       " 'потім',\n",
       " 'проти',\n",
       " 'після',\n",
       " 'років',\n",
       " 'самим',\n",
       " 'самих',\n",
       " 'самій',\n",
       " 'свого',\n",
       " 'своєї',\n",
       " 'своїх',\n",
       " 'собою',\n",
       " 'справ',\n",
       " 'такий',\n",
       " 'також',\n",
       " 'тепер',\n",
       " 'тисяч',\n",
       " 'тобою',\n",
       " 'треба',\n",
       " 'трохи',\n",
       " 'усюди',\n",
       " 'усіма',\n",
       " 'хочеш',\n",
       " 'цього',\n",
       " 'цьому',\n",
       " 'часто',\n",
       " 'через',\n",
       " 'шість',\n",
       " 'якого',\n",
       " 'іноді',\n",
       " 'інший',\n",
       " 'інших',\n",
       " 'багато',\n",
       " 'будемо',\n",
       " 'будете',\n",
       " 'будуть',\n",
       " 'більше',\n",
       " 'всього',\n",
       " 'всьому',\n",
       " 'далеко',\n",
       " 'десять',\n",
       " 'досить',\n",
       " 'другий',\n",
       " 'дійсно',\n",
       " 'завжди',\n",
       " 'звідси',\n",
       " 'зовсім',\n",
       " 'кругом',\n",
       " 'кілька',\n",
       " 'людина',\n",
       " 'можуть',\n",
       " 'навіть',\n",
       " 'навіщо',\n",
       " 'нагорі',\n",
       " 'небудь',\n",
       " 'низько',\n",
       " 'ніколи',\n",
       " 'нікуди',\n",
       " 'нічого',\n",
       " 'обидва',\n",
       " 'одного',\n",
       " 'однієї',\n",
       " \"п'ятий\",\n",
       " 'перший',\n",
       " 'просто',\n",
       " 'раніше',\n",
       " 'раптом',\n",
       " 'самими',\n",
       " 'самого',\n",
       " 'самому',\n",
       " 'сказав',\n",
       " 'скрізь',\n",
       " 'сьомий',\n",
       " 'третій',\n",
       " 'тільки',\n",
       " 'хотіти',\n",
       " 'чотири',\n",
       " 'чудово',\n",
       " 'шостий',\n",
       " 'близько',\n",
       " 'важлива',\n",
       " 'важливе',\n",
       " 'важливі',\n",
       " 'вдалині',\n",
       " 'восьмий',\n",
       " 'говорив',\n",
       " \"дев'ять\",\n",
       " 'десятий',\n",
       " 'зайнята',\n",
       " 'зайнято',\n",
       " 'зайняті',\n",
       " 'занадто',\n",
       " 'значить',\n",
       " 'навколо',\n",
       " 'нарешті',\n",
       " 'нерідко',\n",
       " 'повинно',\n",
       " 'посеред',\n",
       " 'початку',\n",
       " 'пізніше',\n",
       " 'сказала',\n",
       " 'сказати',\n",
       " 'скільки',\n",
       " 'спасибі',\n",
       " 'частіше',\n",
       " 'важливий',\n",
       " 'двадцять',\n",
       " \"дев'ятий\",\n",
       " 'зазвичай',\n",
       " 'зайнятий',\n",
       " 'звичайно',\n",
       " 'здається',\n",
       " 'найбільш',\n",
       " 'не можна',\n",
       " 'недалеко',\n",
       " 'особливо',\n",
       " 'потрібно',\n",
       " 'спочатку',\n",
       " 'сьогодні',\n",
       " 'численна',\n",
       " 'численне',\n",
       " 'численні',\n",
       " 'відсотків',\n",
       " 'двадцятий',\n",
       " 'звідусіль',\n",
       " 'мільйонів',\n",
       " 'нещодавно',\n",
       " 'прекрасно',\n",
       " 'четвертий',\n",
       " 'численний',\n",
       " 'будь ласка',\n",
       " 'дванадцять',\n",
       " 'одинадцять',\n",
       " 'сімнадцять',\n",
       " 'тринадцять',\n",
       " 'безперервно',\n",
       " 'дванадцятий',\n",
       " 'одинадцятий',\n",
       " 'одного разу',\n",
       " \"п'ятнадцять\",\n",
       " 'сімнадцятий',\n",
       " 'тринадцятий',\n",
       " 'шістнадцять',\n",
       " 'вісімнадцять',\n",
       " \"п'ятнадцятий\",\n",
       " 'чотирнадцять',\n",
       " 'шістнадцятий',\n",
       " 'вісімнадцятий',\n",
       " \"дев'ятнадцять\",\n",
       " 'чотирнадцятий',\n",
       " \"дев'ятнадцятий\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.14999999999999 %\n"
     ]
    }
   ],
   "source": [
    "#Compute how many words in example sentence is NOT in the stopwords list:\n",
    "\n",
    "def content_fraction(text):\n",
    "    stopwords = get_stop_words('ukrainian')\n",
    "    content = [w for w in text if w.lower() not in stopwords]\n",
    "    return round((len(content) / len(text)), 4)\n",
    "tok = nltk.word_tokenize(t)\n",
    "print(content_fraction(tok)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bag-of-Words Approach (CountVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more often a word/token appears in a document, the more important it is.\n",
    "\n",
    "**PROS:** Easy to use and understand.\n",
    "Built-in many scientific/NLP libraries.\n",
    "Memory-efficient sparse format.\n",
    "Works well enough.\n",
    "\n",
    "**CONS:** \n",
    "Huge corpus usually leads to huge vocabulary size.\n",
    "Doesn't catch details (semantics, relations, structure etc.).\n",
    "Orderless representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 TF-IDF (TfIdfVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-words approach that is slightly modified: \n",
    "If a word/token appears in a document, but rarely appears in other documents - it is important and vice versa: \n",
    "if its commonly across most documents - then we cannot rely on this word to help us distinquish between texts\n",
    "\n",
    "\n",
    "**Mostly used Parameters:**\n",
    "* **analyzer**={‘word’, ‘char’, ‘char_wb’} - what token to use (word, char-n-grams etc.)\n",
    "* **ngram_range**=(min_n, max_n) - what N to use: say, ngram_range=(1,2) $\\rightarrow$  use both unigrams and bigrams\n",
    "* **stop_words**={‘english’, list_of_words, or None} (default) - whether to filter stop-words or not\n",
    "* **max_features**={N, None} - to build a vocabulary that only consider the top N ordered by term frequency across the corpus\n",
    "* **norm**={‘l1’, ‘l2’ or None, optional} - norm feature vector to unit norm ($L_2-$, $L_1-$ norms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Hashes (HashingVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This vectorizer implementation uses the hashing trick to find the mapping of token string name to feature integer index.\n",
    "\n",
    "**PROS:** \n",
    "Very memory-scalable to large datasets as there is no need to store a vocabulary dictionary in memory\n",
    "\n",
    "**CONS:**\n",
    "There is no way to compute the inverse transform (to get from feature indices to string feature names) \n",
    "which can be a problem when trying to introspect which features are most important to a model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mostly used Parameters:**\n",
    "* **analyzer**={‘word’, ‘char’, ‘char_wb’} - what token to use (word, char-n-grams etc.)\n",
    "* **ngram_range**=(min_n, max_n) - what N to use: say, ngram_range=(1,2) $\\rightarrow$  use both unigrams and bigrams\n",
    "* **stop_words**={‘english’, list_of_words, or None} (default) - whether to filter stop-words or not\n",
    "* **n_features**={N} - how many \"buckets\" to use\n",
    "* **norm**={‘l1’, ‘l2’ or None, optional} - norm feature vector to unit norm ($L_2-$, $L_1-$ norms)\n",
    "* **non_negative**={True,False} whether to use non-negative values only (othervise, they will be centered around 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(analyzer='word', \n",
    "                    ngram_range=(1,4),\n",
    "                    vocabulary=None, # or vocabulary=your_own_dictionary\n",
    "                    max_df=0.7, # don't filter words by their frequency\n",
    "                    max_features=20 )\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 5),\n",
    "                              analyzer = 'word', binary = True, max_df = 0.5, vocabulary=None)\n",
    "\n",
    "tfidf_vectorizer_modified = TfidfVectorizer(ngram_range=(1, 5),\n",
    "                              analyzer = 'char_wb', binary = True, max_df = 0.6, vocabulary=None)\n",
    "\n",
    "hash_vectorizer = HashingVectorizer(\n",
    "                    analyzer='word', # token = word\n",
    "                    ngram_range=(1,4)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary for feature extraction:\n",
    "#### from all vectorizers TF-idf is widely used for different tasks and after some modelling I saw that results for each model are best with this method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alpha - additive (Laplace/Lidstone) smoothing parameter\n",
    "\n",
    "Naive Bayes is the simplest form of Bayesian network, in which all attributes are independent given the value of the class variable. This is called conditional independence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation accuracy on logistic regression: 0.988561076605\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB(alpha=0.000001) # almost no smoothing\n",
    "\n",
    "pipeline_nb = Pipeline([('vectorizer', tfidf_vectorizer),\n",
    "                     ('clf_svm', nb)])\n",
    "\n",
    "# Cross-validation\n",
    "scores_nb = cross_val_score(pipeline_nb, train.Text, train.band, cv=4, n_jobs=-1, scoring=\"roc_auc\")\n",
    "print('Cross validation accuracy on logistic regression:', scores_nb.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline_nb.fit(train[\"Text\"],train[\"band\"])\n",
    "\n",
    "predicted_nb = pipeline_nb.predict_proba(test[\"Text\"])[:,1]\n",
    "y_pred_nb = [0 if pred < 0.5 else 1 for pred in predicted_nb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test accuracy:  97.619047619%\n"
     ]
    }
   ],
   "source": [
    "print('Total test accuracy: ', str(accuracy_score(test['band'], y_pred_nb)*100)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation accuracy on logistic regression: 0.997412008282\n"
     ]
    }
   ],
   "source": [
    "# Check Logistic Regression\n",
    "clf_logr = LogisticRegression(C=5)\n",
    "\n",
    "pipeline_logr = Pipeline([('vectorizer', tfidf_vectorizer_modified),\n",
    "                     ('clf_svm', clf_logr)])\n",
    "\n",
    "# Cross-validation\n",
    "scores_logr = cross_val_score(pipeline_logr, train.Text, train.band, cv=4, n_jobs=-1, scoring=\"roc_auc\")\n",
    "print('Cross validation accuracy on logistic regression:', scores_logr.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline_logr.fit(train[\"Text\"],train[\"band\"])\n",
    "\n",
    "predicted_logr = pipeline_logr.predict_proba(test[\"Text\"])[:,1]\n",
    "y_pred_logr = [0 if pred < 0.5 else 1 for pred in predicted_logr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test accuracy:  97.619047619%\n"
     ]
    }
   ],
   "source": [
    "print('Total test accuracy: ', str(accuracy_score(test['band'], y_pred_logr)*100)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM (Best results!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation accuracy on svm: 0.997412008282\n"
     ]
    }
   ],
   "source": [
    "# Create SVM model \n",
    "svm = LinearSVC(C=1)\n",
    "clf_svm = CalibratedClassifierCV(svm) \n",
    "\n",
    "pipeline_svm = Pipeline([('vectorizer', tfidf_vectorizer_modified),\n",
    "                     ('clf_svm', clf_svm)])\n",
    "\n",
    "# Cross-validation\n",
    "scores_svm = cross_val_score(pipeline_svm, train.Text, train.band, cv=4, n_jobs=-1, scoring=\"roc_auc\")\n",
    "print('Cross validation accuracy on svm:', scores_svm.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline_svm.fit(train[\"Text\"],train[\"band\"])\n",
    "\n",
    "predicted_svm = pipeline_svm.predict_proba(test[\"Text\"])[:,1]\n",
    "y_pred_svm = [0 if pred < 0.5 else 1 for pred in predicted_svm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test accuracy:  100.0%\n"
     ]
    }
   ],
   "source": [
    "print('Total test accuracy: ', str(accuracy_score(test['band'], y_pred_svm)*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble. RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also tried RandomForest model to compare results achieved with linear modelling. \n",
    "\n",
    "At the end, Random Forest as an ensemble learning method gave much worse results than linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = TextBlob(text)\n",
    "    return text.words\n",
    "\n",
    "# split data on train and test \n",
    "X_train, X_val, y_train, y_val  = train_test_split(\n",
    "        train['Text'], \n",
    "        train['band'],\n",
    "        test_size=0.1, \n",
    "        random_state=42)\n",
    "\n",
    "#transform text to feature vectors\n",
    "train_hash = tfidf_vectorizer_modified.fit_transform(X_train)         #train data\n",
    "valid_hash = tfidf_vectorizer_modified.transform(X_val)          #validation data\n",
    "test_hash = tfidf_vectorizer_modified.transform(test['Text'])   #test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.833333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "clf_rf = RandomForestClassifier(n_estimators = 100, \n",
    "                                       min_samples_leaf=5, \n",
    "                                       random_state = 17,\n",
    "                                       class_weight='balanced',\n",
    "                                       verbose=1\n",
    "                                      )\n",
    "model_gbm = clf_rf.fit(X=train_hash, y=y_train)\n",
    "y_pred_gbm = model_gbm.predict(test_hash)\n",
    "print (\"Test Accuracy: \", accuracy_score(test['band'], y_pred_gbm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes: \n",
    "* After using stopwords accuracy became ~7% worse. This is due to the dataset's size which is small. \n",
    "* In practical classification tasks, linear logistic regression and SVMs often give very similar results. Logistic regression tries to maximize the conditional likelihoods of the training data, which makes it more prone to outliers than SVMs. The SVMs mostly care about the points that are closest to the decision boundary (support vectors).\n",
    "\n",
    "# Summary:\n",
    "* Linear models work pretty good for NLP tasks, however, well tuned RNN will work better, but there is no reason to use neural network in this particular small task.\n",
    "* Bag-of-words is very simple method of extracting features, still gives good results. \n",
    "* After testing and tuning several vectorizors I stopped at TF-idf approach as it provided best accuracy on both validation and test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}